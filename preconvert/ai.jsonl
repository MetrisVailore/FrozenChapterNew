{"question": "Why should we train a tiny GPT?", "answer": "Training a tiny GPT helps us understand how transformer models work, including tokenization, embedding, attention mechanisms, and sequence generation. It is useful for learning and experimentation on small datasets."}
{"question": "What does the Length‑MAX tokenizer do?", "answer": "The Length‑MAX tokenizer scans text for all substrings up to a maximum length and counts their frequency. It selects the most frequent substrings as tokens, enabling efficient encoding of text with overlapping patterns."}
{"question": "What is the purpose of AI?", "answer": "The purpose of AI is to automate intelligent behavior, assist humans, and solve complex problems that are difficult to program manually."}
{"question": "Why is text generation creative?", "answer": "Text generation appears creative because models combine learned patterns and knowledge to produce coherent, novel outputs."}
{"question": "How can AI help society?", "answer": "AI can help society by improving healthcare, automating repetitive tasks, enhancing education, and enabling scientific discoveries."}
{"question": "Why are short sequences important?", "answer": "Short sequences make training faster and reduce memory consumption. They also simplify batching and padding, which are crucial when working with limited GPU resources."}
{"question": "How do padded batches work?", "answer": "Padding adds a special token to make all sequences in a batch the same length. This allows parallel processing in GPUs and ensures that loss computation aligns correctly across sequences."}
{"question": "How should you greet AI?", "answer": "A simple greeting like 'Hello AI, how are you?' can be used for testing chatbot responses or training conversational models. It helps models learn typical question-answer interactions."}
{"question": "What should we train GPT on?", "answer": "We can train GPT on small datasets containing varied text, such as programming examples, short stories, or QA pairs. Even limited data is useful for testing model architecture and training pipelines."}
{"question": "What can tiny GPT generate?", "answer": "Tiny GPT can generate short sentences, complete phrases, or answer simple questions. While it may not produce perfect long-form text, it can still demonstrate the underlying generative capabilities of transformers."}
{"question": "Why Python is preferred for AI?", "answer": "Python is widely used for AI because of its simplicity, readability, and rich ecosystem of libraries like PyTorch, TensorFlow, and HuggingFace. It allows rapid experimentation and prototyping."}
{"question": "What is machine learning?", "answer": "Machine learning is a branch of AI focused on algorithms that can learn patterns from data and make predictions. It includes supervised learning, unsupervised learning, and reinforcement learning."}
{"question": "How do deep learning models work?", "answer": "Deep learning models use multiple layers of neurons to learn hierarchical features from data. They are especially effective for tasks like image recognition, NLP, and speech processing."}
{"question": "How are AI, ML, and data science related?", "answer": "Data science combines statistical analysis, ML, and AI to extract insights from data. AI provides models for decision-making, while ML offers algorithms to learn from patterns automatically."}
{"question": "What is the role of transformers in NLP?", "answer": "Transformers revolutionized NLP by introducing self-attention, enabling models to process sequences in parallel and capture long-range dependencies effectively. They are the backbone of GPT and BERT models."}
{"question": "What do tokenizers do?", "answer": "Tokenizers convert raw text into numerical sequences that models can process. They split text into meaningful units, map tokens to IDs, and sometimes handle special cases like unknown words."}
{"question": "Why use tiny models?", "answer": "Tiny models train faster, require less memory, and are easier to experiment with. They are useful for debugging, teaching, and proof-of-concept projects."}
{"question": "Why is batching important?", "answer": "Batching combines multiple sequences into a single computational pass, improving GPU efficiency and stabilizing gradient updates during training."}
{"question": "How does padding affect model training?", "answer": "Padding ensures all sequences in a batch have the same length, preventing shape mismatches. Models ignore padding tokens during attention calculations to avoid learning from them."}
{"question": "What is the purpose of token IDs?", "answer": "Token IDs are numerical representations of tokens. They allow models to work with discrete inputs while preserving semantic meaning through embeddings."}
{"question": "What topics are trending in AI?", "answer": "Current trending topics include large language models, reinforcement learning, generative AI, computer vision, and ethical AI practices. Researchers also focus on efficiency and multimodal learning."}
{"question": "Why experiment with hyperparameters?", "answer": "Hyperparameters like learning rate, batch size, and sequence length significantly affect training quality. Experimenting helps find the optimal configuration for faster convergence and better performance."}
{"question": "What is overfitting?", "answer": "Overfitting occurs when a model learns the training data too well, including noise, leading to poor generalization on unseen data. Techniques like regularization, dropout, and validation sets help prevent it."}
{"question": "How to evaluate model outputs?", "answer": "Outputs can be evaluated using metrics like accuracy, perplexity, BLEU scores, or human evaluation. Comparing generated text with reference answers ensures quality and coherence."}
{"question": "Why save checkpoints?", "answer": "Checkpoints save model state during training, allowing recovery from crashes, experimenting with different settings, or fine-tuning later without starting from scratch."}
{"question": "What is fine-tuning?", "answer": "Fine-tuning adjusts a pre-trained model on a smaller dataset for a specific task. It is faster than training from scratch and often improves performance on domain-specific tasks."}
{"question": "How do learning rates affect training?", "answer": "Learning rates control how much model weights are updated during training. Too high can cause divergence, too low can slow convergence. Scheduling or warm-up strategies can help."}
{"question": "Why use GPUs for training?", "answer": "GPUs accelerate matrix operations in deep learning, enabling faster training on large models. Using GPUs reduces training time from hours to minutes for small datasets."}
{"question": "How do neural networks learn patterns?", "answer": "Neural networks learn patterns by adjusting weights based on input-output examples using backpropagation. Layers capture progressively more abstract features."}
{"question": "What is text generation?", "answer": "Text generation is the process of producing coherent text from a model, often conditioned on a prompt. It relies on learned patterns in data and attention mechanisms in transformers."}
{"question": "What is the difference between deep, shallow, and tiny models?", "answer": "Deep models have many layers and high capacity, shallow models have few layers, and tiny models are small with limited parameters. Each has trade-offs in performance and resource usage."}
{"question": "Why debug models?", "answer": "Debugging helps identify issues in data preprocessing, tokenization, model architecture, or training loops. Proper debugging prevents wasted computation and improves model performance."}
{"question": "Why are small datasets useful?", "answer": "Small datasets allow fast iteration, teaching, and experimentation. They help verify code and pipelines before scaling up to larger datasets."}
{"question": "How do CNNs, RNNs, and transformers differ?", "answer": "CNNs are good for spatial patterns, RNNs for sequential data, and transformers for long-range dependencies and parallel processing. Each architecture is suited for different tasks."}
{"question": "Why clean and preprocess data?", "answer": "Preprocessing ensures consistent formatting, removes noise, handles missing values, and tokenizes text correctly, which is crucial for model performance."}
{"question": "What are batch size, sequence length, and epochs?", "answer": "Batch size is the number of samples per training step, sequence length is the maximum tokens per input, and epochs are full passes over the dataset. They control training efficiency and convergence."}
{"question": "How do models learn from context?", "answer": "Models use embeddings and attention to relate tokens to each other, capturing context and semantic meaning to predict the next token or answer questions."}
{"question": "Why is text generation creative?", "answer": "Text generation can combine patterns, synonyms, and structures learned from data to produce novel sequences, making it appear creative or human-like."}
{"question": "Which hardware should be used?", "answer": "GPUs, CPUs, or TPUs can be used depending on availability. GPUs are preferred for deep learning due to parallel computation efficiency."}
{"question": "What is the purpose of greetings like 'Hello world!'?", "answer": "Simple greetings are often used as sanity checks to ensure models or scripts run correctly. They are also common in tutorials and examples."}
{"question": "How can tiny GPT models be used?", "answer": "Tiny GPT models are useful for experimenting with generative tasks, understanding transformers, and performing small-scale text generation or QA tasks."}
{"question": "Which libraries help train models?", "answer": "Libraries like PyTorch and HuggingFace provide tools for model creation, training, tokenization, and evaluation, making development faster and easier."}
{"question": "Why are token IDs important?", "answer": "Token IDs convert text into numerical form for the model, allowing embeddings to capture semantic relationships while enabling sequence processing."}
{"question": "How does padding affect uniformity?", "answer": "Padding ensures sequences in a batch are the same length, preventing shape mismatches and enabling batch-wise parallel computation."}
{"question": "Why are transformers important in NLP?", "answer": "Transformers capture long-range dependencies, allow parallel sequence processing, and have led to state-of-the-art models in many NLP tasks."}
